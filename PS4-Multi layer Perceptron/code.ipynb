{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "avsaoji_34386391_ps4",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "da5259348fde4a268f64cc0c276fef95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c22608ba478f4ead8c2384019e97f109",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6d8b3a80dbd746d183c0b899ad2038d2",
              "IPY_MODEL_861a8a28580f4948880e60ffcfaf6c7b"
            ]
          }
        },
        "c22608ba478f4ead8c2384019e97f109": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6d8b3a80dbd746d183c0b899ad2038d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ea664cf4baf041dbab12fcabfcad64f4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_880276d0855144a2971161c28d8e5d9f"
          }
        },
        "861a8a28580f4948880e60ffcfaf6c7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_61af4fbf31484e22a76de216ee473952",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "170500096it [00:04, 40649085.22it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4580940c41da445fad523d7c46e3b0ce"
          }
        },
        "ea664cf4baf041dbab12fcabfcad64f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "880276d0855144a2971161c28d8e5d9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "61af4fbf31484e22a76de216ee473952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4580940c41da445fad523d7c46e3b0ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix5dQS2rUMlu",
        "colab_type": "text"
      },
      "source": [
        "#EECS 504 PS4: Backpropagation\n",
        "\n",
        "Please provide the following information \n",
        "(e.g. Andrew Owens, ahowens):\n",
        "\n",
        "Aaditi Saoji, avsaoji\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Cst4k4tuBc",
        "colab_type": "text"
      },
      "source": [
        "# Starting\n",
        "\n",
        "Run the following code to import the modules you'll need. After your finish the assignment, remember to run all cells and save the note book to your local machine as a .ipynb file for Canvas submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHumIO-xt57H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "da5259348fde4a268f64cc0c276fef95",
            "c22608ba478f4ead8c2384019e97f109",
            "6d8b3a80dbd746d183c0b899ad2038d2",
            "861a8a28580f4948880e60ffcfaf6c7b",
            "ea664cf4baf041dbab12fcabfcad64f4",
            "880276d0855144a2971161c28d8e5d9f",
            "61af4fbf31484e22a76de216ee473952",
            "4580940c41da445fad523d7c46e3b0ce"
          ]
        },
        "outputId": "c6b14598-fe0d-4a9b-a4d8-7eb2b6976733"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torchvision.datasets import CIFAR10\n",
        "download = not os.path.isdir('cifar-10-batches-py')\n",
        "dset_train = CIFAR10(root='.', download=download)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da5259348fde4a268f64cc0c276fef95",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./cifar-10-python.tar.gz to .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apEPzDNtK0MC",
        "colab_type": "text"
      },
      "source": [
        "# Problem 4.2 Multi-layer perceptron\n",
        "In this problem you will develop a two Layer neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n",
        "\n",
        "We train the network with a softmax loss function on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. In other words, the network has the following architecture:\n",
        "\n",
        "input - fully connected layer - ReLU - fully connected layer - softmax\n",
        "\n",
        "The outputs of the second fully-connected layer are the scores for each class.\n",
        "\n",
        "You cannot use any deep learning libraries such as PyTorch in this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXfumCQ21JoK",
        "colab_type": "text"
      },
      "source": [
        "# 4.2 (a) Layers\n",
        "In this problem, implement fully connected layer, relu and softmax. Filling in all TODOs in skeleton codes will be sufficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ljfgMv9PHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fc_forward(x, w, b):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a fully-connected layer.\n",
        "    \n",
        "    The input x has shape (N, Din) and contains a minibatch of N\n",
        "    examples, where each example x[i] has shape (Din,).\n",
        "    \n",
        "    Inputs:\n",
        "    - x: A numpy array containing input data, of shape (N, Din)\n",
        "    - w: A numpy array of weights, of shape (Din, Dout)\n",
        "    - b: A numpy array of biases, of shape (Dout,)\n",
        "    \n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, Dout)\n",
        "    - cache: (x, w, b)\n",
        "    \"\"\"\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the forward pass. Store the result in out.              #\n",
        "    ###########################################################################\n",
        "    out = np.dot(x,w) + b\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = (x, w, b)\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def fc_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a fully_connected layer.\n",
        "    \n",
        "    Inputs:\n",
        "    - dout: Upstream derivative, of shape (N, Dout)\n",
        "    - cache: returned by your forward function. Tuple of:\n",
        "      - x: Input data, of shape (N, Din)\n",
        "      - w: Weights, of shape (Din, Dout)\n",
        "      - b: Biases, of shape (Dout,)\n",
        "      \n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to x, of shape (N, Din)\n",
        "    - dw: Gradient with respect to w, of shape (Din, Dout)\n",
        "    - db: Gradient with respect to b, of shape (Dout,)\n",
        "    \"\"\"\n",
        "    x, w, b = cache\n",
        "    dx, dw, db = None, None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the affine backward pass.                               #\n",
        "    ###########################################################################\n",
        "    N,Din = x.shape \n",
        "    dx = np.dot(dout,w.T)      \n",
        "    dw = np.dot(x.T, dout)   \n",
        "    t = np.ones(N)     \n",
        "    db = np.dot(dout.T, t)\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx, dw, db\n",
        "\n",
        "def relu_forward(x):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - x: Inputs, of any shape\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output, of the same shape as x\n",
        "    - cache: x\n",
        "    \"\"\"\n",
        "    out = x\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU forward pass.                                  #\n",
        "    ###########################################################################\n",
        "\n",
        "    out = np.maximum(x,0)\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = x\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def relu_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - dout: Upstream derivatives, of any shape\n",
        "    - cache: returned by your forward function. Input x, of same shape as dout\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    dx, x = dout, cache\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU backward pass.                                 #\n",
        "    ###########################################################################\n",
        "    dx[x <= 0] = 0    \n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx\n",
        "\n",
        "\n",
        "def softmax_loss(x, y):\n",
        "    \"\"\"\n",
        "    Computes the loss and gradient for softmax classification.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
        "      class for the ith input.\n",
        "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
        "      0 <= y[i] < C\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss: Scalar giving the loss\n",
        "    - dx: Gradient of the loss with respect to x\n",
        "    \"\"\"\n",
        "    loss, dx = None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement softmax loss                                            #\n",
        "    ###########################################################################\n",
        "\n",
        "    soft = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    soft = soft/(np.sum(soft, axis=1, keepdims=True))\n",
        "    N,C = x.shape\n",
        "    m = soft[np.arange(N), y]\n",
        "    loss = (-1 * np.sum(np.log(m))) / N\n",
        "    dx = soft.copy()\n",
        "    dx[np.arange(N), y] = dx[np.arange(N), y] - 1\n",
        "    dx = dx/N\n",
        "    \n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return loss, dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbFxtS3zK8oz",
        "colab_type": "text"
      },
      "source": [
        "# 4.2 (b) Softmax Classifier\n",
        "\n",
        "In this problem, implement softmax classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytvxbx9UpxVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SoftmaxClassifier(object):\n",
        "    \"\"\"\n",
        "    A fully-connected neural network with\n",
        "    softmax loss that uses a modular layer design. We assume an input dimension\n",
        "    of D, a hidden dimension of H, and perform classification over C classes.\n",
        "\n",
        "    The architecture should be fc - relu - fc - softmax with one hidden layer\n",
        "\n",
        "    The learnable parameters of the model are stored in the dictionary\n",
        "    self.params that maps parameter names to numpy arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=3072, hidden_dim=300, num_classes=10,\n",
        "                 weight_scale=1e-3):\n",
        "        \"\"\"\n",
        "        Initialize a new network.\n",
        "\n",
        "        Inputs:\n",
        "        - input_dim: An integer giving the size of the input\n",
        "        - hidden_dim: An integer giving the size of the hidden layer, None\n",
        "          if there's no hidden layer.\n",
        "        - num_classes: An integer giving the number of classes to classify\n",
        "        - weight_scale: Scalar giving the standard deviation for random\n",
        "          initialization of the weights.\n",
        "        \"\"\"\n",
        "        self.params = {}\n",
        "        ############################################################################\n",
        "        # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n",
        "        # should be initialized from a Gaussian centered at 0.0 with               #\n",
        "        # standard deviation equal to weight_scale, and biases should be           #\n",
        "        # initialized to zero. All weights and biases should be stored in the      #\n",
        "        # dictionary self.params, with fc weights and biases using the keys        #\n",
        "        # 'W' and 'b', i.e., W1, b1 for the weights and bias in the first linear   #\n",
        "        # layer, W2, b2 for the weights and bias in the second linear layer.       #\n",
        "        ############################################################################\n",
        "        #self.hidden_dim = \n",
        "        self.params['W1'] = np.random.randn(input_dim, hidden_dim) * weight_scale \n",
        "        self.params['b1'] = np.zeros(hidden_dim)\n",
        "        self.params['W2'] = np.random.randn(hidden_dim, num_classes) * weight_scale\n",
        "        self.params['b2'] = np.zeros(num_classes)\n",
        "\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "\n",
        "    def forwards_backwards(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Compute loss and gradient for a minibatch of data.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Array of input data of shape (N, Din)\n",
        "        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
        "\n",
        "        Returns:\n",
        "        If y is None, then run a test-time forward pass of the model and return:\n",
        "        - scores: Array of shape (N, C) giving classification scores, where\n",
        "          scores[i, c] is the classification score for X[i] and class c.\n",
        "\n",
        "        If y is not None, then run a training-time forward and backward pass. And\n",
        "        return a tuple of:\n",
        "        - loss: Scalar value giving the loss\n",
        "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
        "          names to gradients of the loss with respect to those parameters.\n",
        "        \"\"\"\n",
        "        scores = None\n",
        "        ############################################################################\n",
        "        # TODO: Implement the forward pass for the two-layer net, computing the    #\n",
        "        # class scores for X and storing them in the scores variable.              #\n",
        "        ############################################################################\n",
        "        \n",
        "        f1_out, f1_cache = fc_forward(X, self.params['W1'], self.params['b1'])\n",
        "        r_out, r_cache = relu_forward(f1_out)\n",
        "        f2_out, f2_cache = fc_forward(r_out, self.params['W2'], self.params['b2'])\n",
        "        scores = f2_out\n",
        "\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "        # If y is None then we are in test mode so just return scores\n",
        "        if y is None:\n",
        "            return scores\n",
        "\n",
        "        loss, grads = 0, {}\n",
        "        ############################################################################\n",
        "        # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n",
        "        # in the loss variable and gradients in the grads dictionary. Compute data #\n",
        "        # loss using softmax, and make sure that grads[k] holds the gradients for  #\n",
        "        # self.params[k].                                                          # \n",
        "        ############################################################################\n",
        "\n",
        "        loss, d_sc = softmax_loss(scores, y)\n",
        "        d_bck2, grads['W2'], grads['b2'] = fc_backward(d_sc, f2_cache)\n",
        "        d_bck_relu = relu_backward(d_bck2, r_cache)\n",
        "        d_bck1, grads['W1'], grads['b1'] = fc_backward(d_bck_relu, f1_cache)\n",
        "        \n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "        return loss, grads\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwp0waIL1h_e",
        "colab_type": "text"
      },
      "source": [
        "# 4.2(c) Training\n",
        "\n",
        "In this problem, you need to preprocess the images and set up model hyperparameters. Notice that adjust the training and val split is optional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZPtQzXGMoCg",
        "colab_type": "code",
        "outputId": "d90afd27-790e-4ab7-8846-c4f610b46a49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding=\"latin1\")\n",
        "    return dict\n",
        "\n",
        "def load_cifar10():\n",
        "    data = {}\n",
        "    meta = unpickle(\"cifar-10-batches-py/batches.meta\")\n",
        "    batch1 = unpickle(\"cifar-10-batches-py/data_batch_1\")\n",
        "    batch2 = unpickle(\"cifar-10-batches-py/data_batch_2\")\n",
        "    batch3 = unpickle(\"cifar-10-batches-py/data_batch_3\")\n",
        "    batch4 = unpickle(\"cifar-10-batches-py/data_batch_4\")\n",
        "    batch5 = unpickle(\"cifar-10-batches-py/data_batch_5\")\n",
        "    test_batch = unpickle(\"cifar-10-batches-py/test_batch\")\n",
        "    X_train = np.vstack((batch1['data'], batch2['data'], batch3['data'],\\\n",
        "                         batch4['data'], batch5['data']))\n",
        "    Y_train = np.array(batch1['labels'] + batch2['labels'] + batch3['labels'] + \n",
        "                       batch4['labels'] + batch5['labels'])\n",
        "    X_test = test_batch['data']\n",
        "    Y_test = test_batch['labels']\n",
        "    \n",
        "    #Preprocess images here                                     \n",
        "    X_train = (X_train-np.mean(X_train,axis=1,keepdims=True))/np.std(X_train,axis=1,keepdims=True)\n",
        "    X_test = (X_test-np.mean(X_test,axis=1,keepdims=True))/np.std(X_test,axis=1,keepdims=True)\n",
        "\n",
        "    data['X_train'] = X_train[:40000]\n",
        "    data['y_train'] = Y_train[:40000]\n",
        "    data['X_val'] = X_train[40000:]\n",
        "    data['y_val'] = Y_train[40000:]\n",
        "    data['X_test'] = X_test\n",
        "    data['y_test'] = Y_test\n",
        "    return data\n",
        "\n",
        "def test_network(model, X, y, num_samples=None, batch_size=100):\n",
        "    \"\"\"\n",
        "    Check accuracy of the model on the provided data.\n",
        "\n",
        "    Inputs:\n",
        "    - model: Image classifier\n",
        "    - X: Array of data, of shape (N, d_1, ..., d_k)\n",
        "    - y: Array of labels, of shape (N,)\n",
        "    - num_samples: If not None, subsample the data and only test the model\n",
        "      on num_samples datapoints.\n",
        "    - batch_size: Split X and y into batches of this size to avoid using\n",
        "      too much memory.\n",
        "\n",
        "    Returns:\n",
        "    - acc: Scalar giving the fraction of instances that were correctly\n",
        "      classified by the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Subsample the data\n",
        "    N = X.shape[0]\n",
        "    if num_samples is not None and N > num_samples:\n",
        "        mask = np.random.choice(N, num_samples)\n",
        "        N = num_samples\n",
        "        X = X[mask]\n",
        "        y = y[mask]\n",
        "\n",
        "    # Compute predictions in batches\n",
        "    num_batches = N // batch_size\n",
        "    if N % batch_size != 0:\n",
        "        num_batches += 1\n",
        "    y_pred = []\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = (i + 1) * batch_size\n",
        "        scores = model.forwards_backwards(X[start:end])\n",
        "        y_pred.append(np.argmax(scores, axis=1))\n",
        "    y_pred = np.hstack(y_pred)\n",
        "    acc = np.mean(y_pred == y)\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train_network(model, data, **kwargs):\n",
        "    \"\"\"\n",
        "     Required arguments:\n",
        "    - model: Image classifier\n",
        "    - data: A dictionary of training and validation data containing:\n",
        "      'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n",
        "      'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n",
        "      'y_train': Array, shape (N_train,) of labels for training images\n",
        "      'y_val': Array, shape (N_val,) of labels for validation images\n",
        "\n",
        "    Optional arguments:\n",
        "    - learning_rate: A scalar for initial learning rate.\n",
        "    - lr_decay: A scalar for learning rate decay; after each epoch the\n",
        "      learning rate is multiplied by this value.\n",
        "    - batch_size: Size of minibatches used to compute loss and gradient\n",
        "      during training.\n",
        "    - num_epochs: The number of epochs to run for during training.\n",
        "    - print_every: Integer; training losses will be printed every\n",
        "      print_every iterations.\n",
        "    - verbose: Boolean; if set to false then no output will be printed\n",
        "      during training.\n",
        "    - num_train_samples: Number of training samples used to check training\n",
        "      accuracy; default is 1000; set to None to use entire training set.\n",
        "    - num_val_samples: Number of validation samples to use to check val\n",
        "      accuracy; default is None, which uses the entire validation set.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    learning_rate =  kwargs.pop('learning_rate', 1e-3)\n",
        "    lr_decay = kwargs.pop('lr_decay', 1.0)\n",
        "    batch_size = kwargs.pop('batch_size', 100)\n",
        "    num_epochs = kwargs.pop('num_epochs', 10)\n",
        "    num_train_samples = kwargs.pop('num_train_samples', 1000)\n",
        "    num_val_samples = kwargs.pop('num_val_samples', None)\n",
        "    print_every = kwargs.pop('print_every', 10)   \n",
        "    verbose = kwargs.pop('verbose', True)\n",
        "    \n",
        "    epoch = 0\n",
        "    best_val_acc = 0\n",
        "    best_params = {}\n",
        "    loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "    \n",
        "    \n",
        "    num_train = data['X_train'].shape[0]\n",
        "    iterations_per_epoch = max(num_train // batch_size, 1)\n",
        "    num_iterations = num_epochs * iterations_per_epoch\n",
        "    \n",
        "\n",
        "    \n",
        "    for t in range(num_iterations):\n",
        "        # Make a minibatch of training data\n",
        "        batch_mask = np.random.choice(num_train, batch_size)\n",
        "        X_batch = data['X_train'][batch_mask]\n",
        "        y_batch = data['y_train'][batch_mask]\n",
        "        \n",
        "        # Compute loss and gradient\n",
        "        loss, grads = model.forwards_backwards(X_batch, y_batch)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # Perform a parameter update\n",
        "        for p, w in model.params.items():\n",
        "            model.params[p] = w - grads[p]*learning_rate\n",
        "          \n",
        "        # Print training loss\n",
        "        if verbose and t % print_every == 0:\n",
        "            print('(Iteration %d / %d) loss: %f' % (\n",
        "                   t + 1, num_iterations, loss_history[-1]))\n",
        "         \n",
        "        # At the end of every epoch, increment the epoch counter and decay\n",
        "        # the learning rate.\n",
        "        epoch_end = (t + 1) % iterations_per_epoch == 0\n",
        "        if epoch_end:\n",
        "            epoch += 1\n",
        "            learning_rate *= lr_decay\n",
        "        \n",
        "        # Check train and val accuracy on the first iteration, the last\n",
        "        # iteration, and at the end of each epoch.\n",
        "        first_it = (t == 0)\n",
        "        last_it = (t == num_iterations - 1)\n",
        "        if first_it or last_it or epoch_end:\n",
        "            train_acc = test_network(model, data['X_train'], data['y_train'],\n",
        "                num_samples= num_train_samples)\n",
        "            val_acc = test_network(model, data['X_val'], data['y_val'],\n",
        "                num_samples=num_val_samples)\n",
        "            train_acc_history.append(train_acc)\n",
        "            val_acc_history.append(val_acc)\n",
        "\n",
        "            if verbose:\n",
        "                print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (\n",
        "                       epoch, num_epochs, train_acc, val_acc))\n",
        "\n",
        "            # Keep track of the best model\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_params = {}\n",
        "                for k, v in model.params.items():\n",
        "                    best_params[k] = v.copy()\n",
        "        \n",
        "    model.params = best_params\n",
        "        \n",
        "    return model, train_acc_history, val_acc_history\n",
        "        \n",
        "\n",
        "# load data\n",
        "data = load_cifar10() \n",
        "train_data = { k: data[k] for k in ['X_train', 'y_train', \n",
        "                                    'X_val', 'y_val']}\n",
        "#######################################################################\n",
        "# TODO: Set up model hyperparameters                                  #\n",
        "#######################################################################\n",
        "\n",
        "# initialize model\n",
        "model = SoftmaxClassifier(hidden_dim =300, weight_scale=1e-2)\n",
        "\n",
        "# start training    \n",
        "model, train_acc_history, val_acc_history = train_network(\n",
        "    model, train_data, learning_rate = 0.001,\n",
        "    lr_decay=1.2, num_epochs=10, \n",
        "    batch_size=30, print_every=1000)\n",
        "#######################################################################\n",
        "#                         END OF YOUR CODE                            #\n",
        "#######################################################################\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Iteration 1 / 13330) loss: 2.293494\n",
            "(Epoch 0 / 10) train acc: 0.090000; val_acc: 0.093200\n",
            "(Iteration 1001 / 13330) loss: 2.089169\n",
            "(Epoch 1 / 10) train acc: 0.317000; val_acc: 0.304200\n",
            "(Iteration 2001 / 13330) loss: 1.668689\n",
            "(Epoch 2 / 10) train acc: 0.373000; val_acc: 0.343600\n",
            "(Iteration 3001 / 13330) loss: 2.068265\n",
            "(Epoch 3 / 10) train acc: 0.370000; val_acc: 0.377400\n",
            "(Iteration 4001 / 13330) loss: 1.602955\n",
            "(Iteration 5001 / 13330) loss: 1.741087\n",
            "(Epoch 4 / 10) train acc: 0.386000; val_acc: 0.400000\n",
            "(Iteration 6001 / 13330) loss: 1.623173\n",
            "(Epoch 5 / 10) train acc: 0.422000; val_acc: 0.419800\n",
            "(Iteration 7001 / 13330) loss: 1.577458\n",
            "(Epoch 6 / 10) train acc: 0.463000; val_acc: 0.439300\n",
            "(Iteration 8001 / 13330) loss: 1.488351\n",
            "(Iteration 9001 / 13330) loss: 1.186751\n",
            "(Epoch 7 / 10) train acc: 0.483000; val_acc: 0.449500\n",
            "(Iteration 10001 / 13330) loss: 1.164556\n",
            "(Epoch 8 / 10) train acc: 0.475000; val_acc: 0.460400\n",
            "(Iteration 11001 / 13330) loss: 1.429405\n",
            "(Epoch 9 / 10) train acc: 0.531000; val_acc: 0.477400\n",
            "(Iteration 12001 / 13330) loss: 1.322997\n",
            "(Iteration 13001 / 13330) loss: 1.191771\n",
            "(Epoch 10 / 10) train acc: 0.543000; val_acc: 0.479500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcovGmpXvXXa",
        "colab_type": "text"
      },
      "source": [
        "# 4.2(c) Report Accuracy\n",
        "\n",
        "Run the given code and report the accuracy on test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwCq8pBhu6dz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "925c9c74-8ff8-4961-8d48-1b4a94442d1d"
      },
      "source": [
        "# report test accuracy\n",
        "acc = test_network(model, data['X_test'], data['y_test'])\n",
        "print(\"Test accuracy: {}\".format(acc))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.4876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTrmbULS7i2N",
        "colab_type": "text"
      },
      "source": [
        "# 4.2(d) Plot\n",
        "\n",
        "Using the train_acc_history and val_acc_history, plot the train & val accuracy versus epochs on one plot. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1EWw6ecptAMC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "22be1002-4880-4681-da19-94d678e34ab9"
      },
      "source": [
        "epochs = np.linspace(0,10, num = 11)\n",
        "a, = plt.plot(epochs, val_acc_history, label = \"Validation Accuracy\")\n",
        "b, = plt.plot(epochs, train_acc_history, label = \"Train Accuracy\")\n",
        "plt.legend(handles=[a,b])\n",
        "plt.xlabel('Epochs')\n",
        "plt.title('Train Accuracy and Validation Accuracy vs Epochs')\n",
        "plt.show(a,b)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEWCAYAAAB2X2wCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}